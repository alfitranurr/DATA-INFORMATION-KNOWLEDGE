{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analisis Sentimen Komentar TikTok Warga Indonesia terhadap #KaburAjaDulu\n",
    "\n",
    "## Pendahuluan\n",
    "Media sosial telah menjadi wadah utama bagi masyarakat untuk mengekspresikan opini, emosi, dan tren yang sedang berkembang. Salah satu tren yang menarik perhatian di TikTok adalah penggunaan tagar #KaburAjaDulu, dimana fenomena #KaburAjaDulu tengah ramai diperbincangkan di media sosial, mencerminkan keinginan masyarakat untuk meninggalkan Indonesia demi bekerja atau melanjutkan studi di luar negeri.\n",
    "\n",
    "Dalam proyek ini, saya melakukan analisis sentimen terhadap komentar warga Indonesia yang menggunakan tagar #KaburAjaDulu di TikTok. Dataset yang digunakan, dikumpulkan menggunakan Apify, sebuah platform web scraping yang memungkinkan ekstraksi data dari berbagai situs, termasuk TikTok.\n",
    "\n",
    "## Tujuan\n",
    "1. Mengidentifikasi pola umum dalam sentimen yang dominan terkait dengan tagar ini.\n",
    "2. Menganalisis kata-kata kunci yang sering muncul dalam komentar untuk memahami persepsi masyarakat terhadap tren ini.\n",
    "\n",
    "Dengan melakukan analisis ini, kita dapat memperoleh wawasan tentang bagaimana tren digital seperti #KaburAjaDulu diterima oleh masyarakat Indonesia, lalu penyebab dari adanya trend #KaburAjaDulu serta dampak emosional yang ditimbulkannya.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\asus\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (2.2.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wordcloud in c:\\users\\asus\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (1.9.4)\n",
      "Requirement already satisfied: numpy>=1.6.1 in c:\\users\\asus\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from wordcloud) (2.2.3)\n",
      "Requirement already satisfied: pillow in c:\\users\\asus\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from wordcloud) (11.1.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\asus\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from wordcloud) (3.10.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\asus\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib->wordcloud) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\asus\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib->wordcloud) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib->wordcloud) (4.56.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\asus\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib->wordcloud) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\asus\\appdata\\roaming\\python\\python313\\site-packages (from matplotlib->wordcloud) (24.2)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\asus\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib->wordcloud) (3.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\asus\\appdata\\roaming\\python\\python313\\site-packages (from matplotlib->wordcloud) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\asus\\appdata\\roaming\\python\\python313\\site-packages (from python-dateutil>=2.7->matplotlib->wordcloud) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pymongo in c:\\users\\asus\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (4.11.1)\n",
      "Requirement already satisfied: dnspython<3.0.0,>=1.16.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pymongo) (2.7.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\asus\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas) (2.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\asus\\appdata\\roaming\\python\\python313\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\asus\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\asus\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\asus\\appdata\\roaming\\python\\python313\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: seaborn in c:\\users\\asus\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (0.13.2)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.20 in c:\\users\\asus\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from seaborn) (2.2.3)\n",
      "Requirement already satisfied: pandas>=1.2 in c:\\users\\asus\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from seaborn) (2.2.3)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in c:\\users\\asus\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from seaborn) (3.10.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\asus\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\asus\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.56.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\asus\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\asus\\appdata\\roaming\\python\\python313\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\asus\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\asus\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\asus\\appdata\\roaming\\python\\python313\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\asus\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas>=1.2->seaborn) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\asus\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas>=1.2->seaborn) (2025.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\asus\\appdata\\roaming\\python\\python313\\site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: emoji in c:\\users\\asus\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (2.14.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install numpy\n",
    "%pip install wordcloud\n",
    "%pip install pymongo\n",
    "%pip install pandas\n",
    "%pip install seaborn\n",
    "%pip install emoji\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from pymongo import MongoClient\n",
    "import emoji\n",
    "import csv\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Koneksi ke MongoDB Atlas\n",
    "# Ganti <db_password> dengan kata sandi MongoDB Anda\n",
    "mongo_uri = \"mongodb+srv://alfitranurr:bookfacepepabri11@cluster0.vfngh.mongodb.net/?retryWrites=true&w=majority&appName=Cluster0\"\n",
    "client = MongoClient(mongo_uri)\n",
    "\n",
    "# Pilih database dan collection\n",
    "db = client['kaburajadulu_db']  # Nama database, bisa diganti sesuai keinginan\n",
    "collection = db['tiktok_posts']  # Nama collection, bisa diganti sesuai keinginan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Fungsi untuk memuat data CSV ke MongoDB\n",
    "def load_csv_to_mongodb(csv_file_path):\n",
    "    # Baca file CSV menggunakan pandas\n",
    "    df = pd.read_csv(csv_file_path)\n",
    "    \n",
    "    # Konversi DataFrame ke list of dictionaries\n",
    "    data = df.to_dict(orient='records')\n",
    "    \n",
    "    # Hapus data lama di collection (opsional, uncomment jika ingin reset)\n",
    "    # collection.delete_many({})\n",
    "    \n",
    "    # Insert data ke MongoDB\n",
    "    collection.insert_many(data)\n",
    "    print(f\"Berhasil memuat {len(data)} dokumen ke MongoDB!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Fungsi untuk membaca data dari MongoDB ke DataFrame\n",
    "def read_from_mongodb():\n",
    "    # Ambil semua data dari collection\n",
    "    data = list(collection.find())\n",
    "    \n",
    "    # Konversi ke DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Hapus kolom '_id' yang otomatis dibuat oleh MongoDB (opsional)\n",
    "    if '_id' in df.columns:\n",
    "        df = df.drop('_id', axis=1)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Berhasil memuat 1250 dokumen ke MongoDB!\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2500 entries, 0 to 2499\n",
      "Data columns (total 3 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   text         2472 non-null   object\n",
      " 1   webVideoUrl  2500 non-null   object\n",
      " 2   createTime   2500 non-null   int64 \n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 58.7+ KB\n",
      "None\n",
      "                                                text  \\\n",
      "0  Simak nih guys! Setelah kemarin heboh, ini dia...   \n",
      "1                               aigooðŸ˜¹#kaburajadulu    \n",
      "2  #mandiriðŸ‡¯ðŸ‡µ #tgmandiriðŸ‡®ðŸ‡©ðŸ‡¯ðŸ‡µ #tokuteginouðŸ‡²ðŸ‡¨ðŸ‡¯ðŸ‡µ #ka...   \n",
      "3  Membalas @arikurni 1 Kasih paham teman-teman, ...   \n",
      "4  berasa paling tinggi,sampai dia lupa diri klo ...   \n",
      "\n",
      "                                         webVideoUrl  createTime  \n",
      "0  https://www.tiktok.com/@metro_tv/video/7473468...  1740052547  \n",
      "1  https://www.tiktok.com/@moonfalls1/video/74701...  1739280024  \n",
      "2  https://www.tiktok.com/@zetrosimanjuntak21/vid...  1740395718  \n",
      "3  https://www.tiktok.com/@babagenz/video/7471111...  1739503720  \n",
      "4  https://www.tiktok.com/@ramly460/video/7474754...  1740351942  \n"
     ]
    }
   ],
   "source": [
    "# 4. Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Ganti dengan path file CSV Anda\n",
    "    csv_file_path = \"dataset_kaburajadulu_raw.csv\"  \n",
    "    \n",
    "    # Load data CSV ke MongoDB \n",
    "    load_csv_to_mongodb(csv_file_path)\n",
    "    \n",
    "    # Baca data dari MongoDB ke DataFrame\n",
    "    df = read_from_mongodb()\n",
    "    \n",
    "    # Tampilkan informasi dasar DataFrame\n",
    "    print(df.info())\n",
    "    print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jumlah duplikat: 2270\n",
      "Baris yang duplikat:\n",
      "                                                   text  \\\n",
      "56    Simak nih guys! Setelah kemarin heboh, ini dia...   \n",
      "57                                 aigooðŸ˜¹#kaburajadulu    \n",
      "58    #mandiriðŸ‡¯ðŸ‡µ #tgmandiriðŸ‡®ðŸ‡©ðŸ‡¯ðŸ‡µ #tokuteginouðŸ‡²ðŸ‡¨ðŸ‡¯ðŸ‡µ #ka...   \n",
      "59    Membalas @arikurni 1 Kasih paham teman-teman, ...   \n",
      "60    berasa paling tinggi,sampai dia lupa diri klo ...   \n",
      "...                                                 ...   \n",
      "2495  semangat para pejuang #kaburajadulu dan juga y...   \n",
      "2496  #xybca #fyppppppppppppppppppppppp #kaburajadul...   \n",
      "2497  Nekat dikit atau kita gak kemana-manaðŸ˜œ. #kabur...   \n",
      "2498  #kaburajadulu #masyaallahtabarakkallah #saudia...   \n",
      "2499  Mengumpulkan uang di negara sendiri menghabisk...   \n",
      "\n",
      "                                            webVideoUrl  createTime  \n",
      "56    https://www.tiktok.com/@metro_tv/video/7473468...  1740052547  \n",
      "57    https://www.tiktok.com/@moonfalls1/video/74701...  1739280024  \n",
      "58    https://www.tiktok.com/@zetrosimanjuntak21/vid...  1740395718  \n",
      "59    https://www.tiktok.com/@babagenz/video/7471111...  1739503720  \n",
      "60    https://www.tiktok.com/@ramly460/video/7474754...  1740351942  \n",
      "...                                                 ...         ...  \n",
      "2495  https://www.tiktok.com/@kitajees/video/7473312...  1740016105  \n",
      "2496  https://www.tiktok.com/@juandaaa_pp/video/7472...  1739776609  \n",
      "2497  https://www.tiktok.com/@adfrmnsyh2/video/74705...  1739370701  \n",
      "2498  https://www.tiktok.com/@cloudqcrush/video/7473...  1739957228  \n",
      "2499  https://www.tiktok.com/@agungpriyambada/video/...  1740470404  \n",
      "\n",
      "[2270 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# Tentukan kolom yang berisi tipe list\n",
    "list_columns = ['text']  # Ganti dengan nama kolom yang sesuai\n",
    "\n",
    "# Mengonversi kolom dengan tipe list menjadi string\n",
    "for col in list_columns:\n",
    "    df[col] = df[col].apply(lambda x: str(x) if isinstance(x, list) else x)\n",
    "\n",
    "# Cek duplikat setelah konversi\n",
    "duplikat = df.duplicated()\n",
    "print(f\"Jumlah duplikat: {duplikat.sum()}\")\n",
    "print(\"Baris yang duplikat:\")\n",
    "print(df[duplikat])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   text  \\\n",
      "0     Simak nih guys! Setelah kemarin heboh, ini dia...   \n",
      "1                                  aigooðŸ˜¹#kaburajadulu    \n",
      "2     #mandiriðŸ‡¯ðŸ‡µ #tgmandiriðŸ‡®ðŸ‡©ðŸ‡¯ðŸ‡µ #tokuteginouðŸ‡²ðŸ‡¨ðŸ‡¯ðŸ‡µ #ka...   \n",
      "3     Membalas @arikurni 1 Kasih paham teman-teman, ...   \n",
      "4     berasa paling tinggi,sampai dia lupa diri klo ...   \n",
      "...                                                 ...   \n",
      "1209  berlindung dari kata kabur aja dulu emang lu d...   \n",
      "1210  efisiensi anggaran sebenernya bagus, cuman kal...   \n",
      "1211  Ada yang mau kabur aja dulu ke Jepang? Langsun...   \n",
      "1230  Sepertinya Indonesia benar2 sedang tidak baik-...   \n",
      "1233  #indonesiagelap #kaburajadulu #koruptor #bunda...   \n",
      "\n",
      "                                            webVideoUrl  createTime  \n",
      "0     https://www.tiktok.com/@metro_tv/video/7473468...  1740052547  \n",
      "1     https://www.tiktok.com/@moonfalls1/video/74701...  1739280024  \n",
      "2     https://www.tiktok.com/@zetrosimanjuntak21/vid...  1740395718  \n",
      "3     https://www.tiktok.com/@babagenz/video/7471111...  1739503720  \n",
      "4     https://www.tiktok.com/@ramly460/video/7474754...  1740351942  \n",
      "...                                                 ...         ...  \n",
      "1209  https://www.tiktok.com/@outfitbobaboza/video/7...  1739442222  \n",
      "1210  https://www.tiktok.com/@mfauzan_ramdani/video/...  1739545414  \n",
      "1211  https://www.tiktok.com/@usamahhuwaidaarofiidah...  1739793616  \n",
      "1230  https://www.tiktok.com/@albahri30_5/video/7473...  1739959252  \n",
      "1233  https://www.tiktok.com/@tehn3ng/video/74752791...  1740474070  \n",
      "\n",
      "[230 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# Menghapus duplikat dari DataFrame\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "# Menampilkan DataFrame setelah duplikat dihapus\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text           2\n",
       "webVideoUrl    0\n",
       "createTime     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Menghapus baris dengan nilai null di kolom 'text'\n",
    "df = df.dropna(subset=['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text           0\n",
      "webVideoUrl    0\n",
      "createTime     0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Periksa kembali jumlah nilai null\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Menyimpan df_cleaned sebagai file CSV\n",
    "df.to_csv('df_cleaned.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 228 entries, 0 to 1233\n",
      "Data columns (total 3 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   text         228 non-null    object\n",
      " 1   webVideoUrl  228 non-null    object\n",
      " 2   createTime   228 non-null    int64 \n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 7.1+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Menghapus semua data lama\n",
    "collection.delete_many({})\n",
    "\n",
    "# Memuat data baru ke MongoDB\n",
    "collection.insert_many(df)  # data_baru adalah data yang telah diproses\n",
    "print(f\"Data berhasil dimuat kembali dengan {len(df)} dokumen.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's Analyze and Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analisis panjang teks\n",
    "df['text_length'] = df['text'].apply(lambda x: len(str(x)))\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.histplot(df['text_length'], bins=30, kde=True, color='blue')\n",
    "plt.xlabel(\"Panjang Teks\")\n",
    "plt.ylabel(\"Frekuensi\")\n",
    "plt.title(\"Distribusi Panjang Teks\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WordCloud untuk melihat kata yang sering muncul\n",
    "text_corpus = \" \".join(df['text'].dropna().astype(str))\n",
    "# Tema warna (misalnya: biru ke ungu)\n",
    "\n",
    "\n",
    "stopwords = set(STOPWORDS)\n",
    "stopwords.update([\"yg\", \"dg\", \"rt\", \"dgn\", \"ny\", \"d\", 'klo',\n",
    "                    'kalo', 'amp', 'biar', 'bikin', 'bilang',\n",
    "                    'gak', 'ga', 'krn', 'nya', 'nih', 'sih',\n",
    "                    'si', 'tau', 'tdk', 'tuh', 'utk', 'ya',\n",
    "                    'jd', 'jgn', 'sdh', 'aja', 'n', 't',\n",
    "                    'nyg', 'hehe', 'pen', 'u', 'nan', 'loh', 'rt',\n",
    "                    'wkwk', 'wkwkwk', 'nggak', 'gakk', 'gk', 'tp', 'lg',\n",
    "                    'bgt', 'dr', 'gw', 'gue', 'gpp', 'aja', 'deh', 'kok',\n",
    "                    'doi', 'banget', 'bangettt', 'cuma', 'kali', 'yaa',\n",
    "                    'udah', 'k', 'sy', 'm', 'bang', 'kak', 'kakak', 'min',\n",
    "                    'mas', 'mba', 'mbak', 'anjay', 'anjir', 'anj', 'lah',\n",
    "                    'lho', 'loh', 'lu', 'loe', 'lo', 'kamu', 'aku', 'gue',\n",
    "                    'gw', 'gua', 'gue', 'kita', 'mereka', 'kalian', 'aku', 'saya',\n",
    "                    'dia', 'mereka', 'kamu', 'kalian', 'kau', 'engkau', 'anda',\n",
    "                    'beliau', 'ia', 'mereka', 'kita', 'ini', 'itu', 'sana', 'sini',\n",
    "                    'mana', 'kapan', 'kenapa', 'bagaimana', 'berapa', 'siapa', 'dimana',\n",
    "                    'apa', 'siapa', 'mana', 'kapan', 'mengapa', 'bagaimana', 'berapa',\n",
    "                    'kabur', 'aja', 'dulu', 'indonesia', 'luar', 'negeri'])\n",
    "\n",
    "wordcloud = WordCloud(width=800,height=400,background_color=\"black\",colormap=\"cool\",stopwords=STOPWORDS,\n",
    "                      max_words=200,contour_width=3,contour_color=\"white\").generate(text_corpus)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Word Cloud Post Tiktok\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "word_counts = Counter(wordcloud.words_)\n",
    "top_10_words = word_counts.most_common(10)\n",
    "print(\"Top 10 words:\")\n",
    "for word, count in top_10_words:\n",
    "    print(f\"{word}: {count:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Berdasarkan analisis dari output top 10 words, terlihat bahwa tema utama diskusi adalah keinginan untuk \"kabur ke luar negeri\", dengan frasa \"kaburajadulu fyp\" menjadi yang paling dominan, mencerminkan dorongan kuat untuk meninggalkan Indonesia, serta penggunaan TikTok sebagai platform utama untuk menyebarkan ide ini. Frasa \"kabur aja\" dan \"aja dulu\" juga cukup sering muncul, mengindikasikan ajakan yang lebih kasual atau ringan, namun tetap mengarah pada keinginan untuk mencari peluang di luar negeri.\n",
    "\n",
    "Kata \"kaburajadulu\" diikuti oleh \"kaburajadulu indonesia\" menunjukkan adanya perbandingan jelas antara kondisi di Indonesia dan di luar negeri, yang memperkuat pesan bahwa banyak orang melihat keluar negeri sebagai tempat yang menawarkan kehidupan atau pekerjaan yang lebih baik. Penggunaan kata \"lebih baik\" juga semakin memperjelas pandangan ini, menggambarkan harapan bahwa kehidupan di luar negeri bisa lebih memadai dari segi peluang.\n",
    "\n",
    "Sementara itu, frasa \"anak muda\" dan \"muda Indonesia\" memberikan gambaran bahwa topik ini sangat relevan dengan generasi muda, yang mungkin merasa bahwa peluang di dalam negeri terbatas. Frasa \"luar negeri\" juga muncul sebagai kata kunci yang menunjukkan destinasi utama yang dibicarakan dalam diskusi ini.\n",
    "\n",
    "Akhirnya, munculnya \"fypã‚· viral\" menegaskan bahwa TikTok berperan penting dalam menyebarkan wacana ini, dengan banyak pengguna yang berusaha membuat konten mereka viral di For You Page (FYP) untuk mendapatkan perhatian lebih luas dan mendukung gagasan \"kabur ke luar negeri\" ini.\n",
    "\n",
    "Secara keseluruhan, analisis ini mengindikasikan bahwa diskusi di platform TikTok banyak difokuskan pada keinginan untuk mencari kehidupan yang lebih baik di luar negeri, dengan TikTok berperan sebagai media untuk menyebarkan ide ini, terutama di kalangan generasi muda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "df['word_count'] = df['text'].apply(lambda x: len(str(x).split()))\n",
    "\n",
    "sns.scatterplot(x=df['text_length'], y=df['word_count'], alpha=0.5, color=\"purple\")\n",
    "plt.xlabel(\"Panjang Teks (Jumlah Karakter)\")\n",
    "plt.ylabel(\"Jumlah Kata\")\n",
    "plt.title(\"Hubungan Panjang Teks dan Jumlah Kata dalam Komentar\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install emoji==2.2.0\n",
    "%pip install emoji==2.14.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the result of demojize\n",
    "\n",
    "df['text'].apply(lambda x: emoji.demojize(x) if isinstance(x, str) else x).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_emojis(text):\n",
    "    return [char for char in text if emoji.is_emoji(char)]\n",
    "\n",
    "all_emojis = [emj for text in df['text'].dropna().astype(str) for emj in extract_emojis(text)]\n",
    "emoji_freq = Counter(all_emojis)\n",
    "top_emojis = emoji_freq.most_common(10)\n",
    "emoji_chars, emoji_counts = zip(*top_emojis)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.barplot(x=list(emoji_chars), y=list(emoji_counts), palette=\"coolwarm\")\n",
    "plt.xlabel(\"Emoji\")\n",
    "plt.ylabel(\"Frekuensi\")\n",
    "plt.title(\"10 Emoji Paling Sering Muncul dalam Komentar\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "%pip install nltk\n",
    "%pip install Sastrawi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import regexp_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the NLTK stopwords list is downloaded\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Membuat stemmer\n",
    "factory = StemmerFactory()\n",
    "stemmer = factory.create_stemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fungsi untuk menghapus URL dan HTML\n",
    "def remove_urls_and_html(text):\n",
    "    # Menghapus URL\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    # Menghapus tag HTML\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fungsi untuk menghapus emoji\n",
    "def remove_emoji(text):\n",
    "    return emoji.replace_emoji(text, replace='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fungsi cleansing teks\n",
    "def cleansing(df):\n",
    "    df_clean = df.astype(str)  # Mengubah semua entri menjadi string\n",
    "    df_clean = df_clean.str.lower()  # Mengubah menjadi huruf kecil\n",
    "    df_clean = [re.sub(r\"\\d+\", \"\", i) for i in df_clean]  # Menghapus angka\n",
    "    df_clean = [re.sub(r'[^\\w]', ' ', i) for i in df_clean]  # Menghapus karakter non-alfanumerik\n",
    "    df_clean = [re.sub(r'\\s+', ' ', i) for i in df_clean]  # Mengganti spasi ganda menjadi satu\n",
    "    df_clean = [i.strip() for i in df_clean]  # Menghapus spasi di awal dan akhir\n",
    "    return df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daftar stopwords Bahasa Indonesia dari NLTK\n",
    "stopwords_list = stopwords.words('indonesian')\n",
    "stopwords_list.extend([\n",
    "    \"yg\", \"dg\", \"rt\", \"dgn\", \"ny\", \"d\", \"klo\", \"kalo\", \"amp\", \"biar\", \"bikin\", \"bilang\", \n",
    "    \"gak\", \"ga\", \"krn\", \"nya\", \"nih\", \"sih\", \"si\", \"tau\", \"tdk\", \"tuh\", \"utk\", \"ya\", \"jd\", \n",
    "    \"jgn\", \"sdh\", \"aja\", \"n\", \"t\", \"nyg\", \"hehe\", \"pen\", \"u\", \"nan\", \"loh\", \"rt\", \"wkwk\", \n",
    "    \"wkwkwk\", \"nggak\", \"gakk\", \"gk\", \"tp\", \"lg\", \"bgt\", \"dr\", \"gw\", \"gue\", \"gpp\", \"aja\", \"deh\", \n",
    "    \"kok\", \"doi\", \"banget\", \"bangettt\", \"cuma\", \"kali\", \"yaa\", \"udah\", \"k\", \"sy\", \"m\", \"bang\", \n",
    "    \"kak\", \"kakak\", \"min\", \"mas\", \"mba\", \"mbak\", \"anjay\", \"anjir\", \"anj\", \"lah\", \"lho\", \"loh\", \n",
    "    \"lu\", \"loe\", \"lo\", \"kamu\", \"aku\", \"gue\", \"gw\", \"gua\", \"gue\", \"kita\", \"mereka\", \"kalian\", \n",
    "    \"aku\", \"saya\", \"dia\", \"mereka\", \"kamu\", \"kalian\", \"kau\", \"engkau\", \"anda\", \"beliau\", \"ia\", \n",
    "    \"mereka\", \"kita\", \"ini\", \"itu\", \"sana\", \"sini\", \"mana\", \"kapan\", \"kenapa\", \"bagaimana\", \n",
    "    \"berapa\", \"siapa\", \"dimana\", \"apa\", \"siapa\", \"mana\", \"kapan\", \"mengapa\", \"bagaimana\", \n",
    "    \"berapa\", \"kabur\", \"aja\", \"dulu\", \"indonesia\", \"luar\", \"negeri\"\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fungsi untuk menghapus stopwords\n",
    "def remove_stopwords(tokens):\n",
    "    return [word for word in tokens if word not in stopwords_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fungsi untuk tokenisasi teks\n",
    "def tokenize_text(text):\n",
    "    return regexp_tokenize(text, pattern=r'\\w+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fungsi utama untuk memproses DataFrame\n",
    "def process_dataframe(df):\n",
    "    df_clean = cleansing(df['text'])\n",
    "    \n",
    "    # Menambahkan kolom 'clean_comment' untuk teks yang sudah dibersihkan\n",
    "    df['clean_comment'] = df_clean\n",
    "    \n",
    "    # Tokenisasi\n",
    "    df['tokens'] = df_clean  # Simpan hasil cleansing langsung ke kolom tokens\n",
    "    \n",
    "    # Tokenisasi teks\n",
    "    df['tokens'] = df['tokens'].apply(tokenize_text)\n",
    "    \n",
    "    # Menghapus stopwords\n",
    "    df['tokens_nostopwords'] = df['tokens'].apply(remove_stopwords)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Proses DataFrame\n",
    "df_processed = process_dataframe(df)\n",
    "\n",
    "# Menampilkan data tokenisasi pada baris pertama\n",
    "print(df_processed['tokens'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the result\n",
    "df['nonstop_comment'] = df['tokens'].apply(lambda x: [word for word in x if word not in stopwords_list])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Make sure stopwords are downloaded\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Get the stopwords from NLTK and convert it to a set\n",
    "stopwords_list = set(stopwords.words('indonesian'))  # Convert to a set to avoid issues with iteration\n",
    "\n",
    "# Combine all the tokens (without stopwords)\n",
    "text_corpus_preprocessed = \" \".join([\" \".join(tokens) for tokens in df['tokens_nostopwords']])\n",
    "\n",
    "# Create the word cloud\n",
    "wordcloud_preprocessed = WordCloud(\n",
    "    width=800, height=400, background_color=\"black\", colormap=\"cool\", \n",
    "    stopwords=stopwords_list, max_words=200, contour_width=3, contour_color=\"white\"\n",
    ").generate(text_corpus_preprocessed)\n",
    "\n",
    "# Plot the wordcloud\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.imshow(wordcloud_preprocessed, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Word Cloud Comment TikTok (Preprocessed)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# InsetLaxiconBased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kamus Leksikon postif dan negatif\n",
    "positive_url = \"https://raw.githubusercontent.com/fajri91/InSet/master/positive.tsv\"\n",
    "negative_url = \"https://raw.githubusercontent.com/fajri91/InSet/master/negative.tsv\"\n",
    "\n",
    "pos_lex = set(pd.read_csv(positive_url, sep='\\t', header=None)[0].str.lower())\n",
    "neg_lex = set(pd.read_csv(negative_url, sep='\\t', header=None)[0].str.lower())\n",
    "\n",
    "# Fungsi untuk menentukan sentimen\n",
    "def determine_sentiment(text):\n",
    "    if isinstance(text, str):\n",
    "        positive_count = sum(1 for word in text.split() if word in pos_lex)\n",
    "        negative_count = sum(1 for word in text.split() if word in neg_lex)\n",
    "        sentiment_score = positive_count - negative_count\n",
    "\n",
    "        if sentiment_score > 0:\n",
    "            sentiment = \"Positif\"\n",
    "        elif sentiment_score < 0:\n",
    "            sentiment = \"Negatif\"\n",
    "        else:\n",
    "            sentiment = \"Netral\"\n",
    "\n",
    "        return sentiment_score, sentiment\n",
    "\n",
    "    return 0, \"Netral\"\n",
    "\n",
    "# Tentukan sentimen dan skor untuk setiap ulasan\n",
    "df[['Score', 'Sentiment']] = df['clean_comment'].apply(lambda x: pd.Series(determine_sentiment(x)))\n",
    "\n",
    "# Tampilkan hasilnya\n",
    "print(df.head(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_count = df['Sentiment'].value_counts()\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "\n",
    "custom_colors = [\"#FFD700\", \"#FF4500\", \"#32CD32\"]\n",
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "ax = sns.barplot(x=sentiment_count.index, y=sentiment_count.values, palette=custom_colors)\n",
    "\n",
    "\n",
    "plt.title(\"Jumlah Analisis Sentimen (Metode InSet Laxicon Based)\", fontsize=14, pad=20)\n",
    "plt.xlabel(\"Class Sentiment\", fontsize=12)\n",
    "plt.ylabel(\"Jumlah Tweets\", fontsize=14)\n",
    "\n",
    "\n",
    "total = len(df['Sentiment'])\n",
    "for i, count in enumerate(sentiment_count.values):\n",
    "    percentage = f\"{100 * count / total:.2f}%\"\n",
    "    ax.text(i, count + 0.10, f\"{count}\\n({percentage})\", ha='center', va='bottom')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.shape)  # Mengecek jumlah data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install scikit-learn\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Pembagian data\n",
    "x_train, x_test, y_train, y_test = train_test_split(df['clean_comment'], df['Sentiment'], test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Representation\n",
    "- Pada tahap ini, menggunakan 2 Text Representation representation dengan 3 Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Representation 1 : BagofWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "x = vectorizer.fit_transform(x_train)\n",
    "train_bow = pd.DataFrame(x.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "# Menampilkan hasil\n",
    "print(\"Representasi Bag of Words:\")\n",
    "print(train_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_bow.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_bow = vectorizer.transform(x_test)\n",
    "test_bow = pd.DataFrame(x_test_bow.toarray(), columns=vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BagOFWords Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf_class = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_class.fit(train_bow, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_rf_class = rf_class.predict(test_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print('\\nRandom Forest Classification Report\\n')\n",
    "print(classification_report(y_test, test_rf_class, target_names=['Negatif','Netral','Positif']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "svm_class1 = svm.LinearSVC( random_state=42)\n",
    "svm_class1.fit(train_bow, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_svm_class1=svm_class1.predict(test_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nClassification Report\\n')\n",
    "print(classification_report(y_test, test_svm_class1, target_names=['Negatif','Netral','Positif']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "nb_class = MultinomialNB()\n",
    "nb_class.fit(train_bow, y_train)\n",
    "test_nb_class = nb_class.predict(test_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nNaive Bayes Classification Report\\n')\n",
    "print(classification_report(y_test, test_nb_class, target_names=['Negatif','Netral','Positif']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Representation 2 : TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X = tfidf_vectorizer.fit_transform(x_train)\n",
    "train_tfidf=pd.DataFrame(X.toarray(),columns=vectorizer.get_feature_names_out())\n",
    "train_tfidf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tfidf = tfidf_vectorizer.transform(x_test)\n",
    "TFIDF_test=pd.DataFrame(test_tfidf.toarray(),columns=tfidf_vectorizer.get_feature_names_out())\n",
    "TFIDF_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF Machine Learning Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf_class2 = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_class2.fit(train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_rf_class2 = rf_class2.predict(test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nRandom Forest Classification TFIDF Report\\n')\n",
    "print(classification_report(y_test, test_rf_class2, target_names=['Negatif','Netral','Positif']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_class2 = svm.LinearSVC( random_state=42)\n",
    "svm_class2.fit(train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_svm_class2 =svm_class2.predict(test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nClassification Report SVM TFIDF\\n')\n",
    "print(classification_report(y_test, test_svm_class2, target_names=['Negatif','Netral','Positif']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "nb_class2 = MultinomialNB()\n",
    "nb_class2.fit(train_tfidf, y_train)\n",
    "test_nb_class2 = nb_class2.predict(test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nNaive Bayes Classification Report\\n')\n",
    "print(classification_report(y_test, test_nb_class2, target_names=['Negatif','Netral','Positif']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Model Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_model(y_test, models_predictions):\n",
    "    best_model = None\n",
    "    best_accuracy = -1\n",
    "\n",
    "    for model_name, predictions in models_predictions.items():\n",
    "        report = classification_report(y_test, predictions, target_names=['Negatif', 'Netral', 'Positif'], output_dict=True)\n",
    "        accuracy = report['accuracy']\n",
    "\n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            best_model = model_name\n",
    "\n",
    "    return best_model, best_accuracy\n",
    "\n",
    "\n",
    "models_predictions = {\n",
    "    'RF_BOW': test_rf_class,\n",
    "    'SVM_BOW': test_svm_class1,\n",
    "    'NB_BOW': test_nb_class,\n",
    "    'RF_TFIDF': test_rf_class2,\n",
    "    'SVM_TFIDF': test_svm_class2,\n",
    "    'NB_TFIDF': test_nb_class2,\n",
    "}\n",
    "\n",
    "best_model, best_accuracy = find_best_model(y_test, models_predictions)\n",
    "print(f\"The best performing model is: {best_model} with accuracy: {best_accuracy:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_predictions = pd.DataFrame({'predictions': test_rf_class})\n",
    "combined_df = pd.concat([df, best_model_predictions], axis=1)\n",
    "\n",
    "sentiment_mapping = {'Negatif': 0, 'Netral': 1, 'Positif': 2}\n",
    "combined_df['Sentiment_num'] = combined_df['Sentiment'].map(sentiment_mapping)\n",
    "combined_df['predictions_num'] = combined_df['predictions'].map(sentiment_mapping)\n",
    "\n",
    "correlation_matrix = combined_df[['Score', 'Sentiment_num', 'predictions_num']].corr()\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=.5)\n",
    "plt.title('Correlation Matrix of Best Model Predictions')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kata yang memengaruhi trend #KaburDuluAja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def get_top_n_words(corpus, n=None):\n",
    "    vec = CountVectorizer().fit(corpus)\n",
    "    bag_of_words = vec.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0)\n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "    return words_freq[:n]\n",
    "\n",
    "top_words_bow = get_top_n_words(df['clean_comment'], n=20)\n",
    "print(\"Top words from Bag of Words:\")\n",
    "for word, freq in top_words_bow:\n",
    "    print(f\"{word}: {freq}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisasi kata berpengaruh\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=[x[1] for x in top_words_bow], y=[x[0] for x in top_words_bow], palette=\"coolwarm\")\n",
    "plt.title(\"Top 20 Words Influencing #KaburAjaDulu Trend (BOW)\")\n",
    "plt.xlabel(\"Frequency\")\n",
    "plt.ylabel(\"Words\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Menghapus semua data lama\n",
    "collection.delete_many({})\n",
    "\n",
    "# Memuat data baru ke MongoDB\n",
    "collection.insert_many(data_baru)  # data_baru adalah data yang telah diproses\n",
    "print(f\"Data berhasil dimuat kembali dengan {len(data_baru)} dokumen.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
